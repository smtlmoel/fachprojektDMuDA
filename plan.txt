Gesamt-Ziel: Kommunikationsaufwand beim federated learning reduzieren

1. Auswirkung Anzahl der Kommunikationsrunden (bei insgesamt gleich vielen Epochen)

    Behauptung: Mehr Kommunikationsrunden bringen höhere Performance. Zu dem Preis eines größeren Datenverkehrs
    WAHR / FALSCH? [ ]
    Fazit: Abwägung höherer Datenverkehr oder mehr Performance

2. Welche Parameter schicken? / Wie viele?
    ⇾ Methoden in Research suchen
    ⇾ Testen und vergleichen: Wie viel schlechter und wie viel weniger Daten? → Lohnt es sich?

    Idee 1: Dropout Gradienten nicht mitschicken [Fazit: sehr schwierig]
    Idee 2: Nur bestimmte schichten schicken → Einträge aus den dictionaries löschen (bestimmte keys)
    Idee 3: Maske → Maske als 0/1 - Matrix und multiplizieren. → Anteil 1sen ist Prozentanteil der geschickten Daten

3. Bei reduziertem Datenverkehr mehr Kommunikationsrunden für eine insgesamt bessere Performance?