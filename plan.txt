Gesamt-Ziel: Kommunikationsaufwand beim federated learning reduzieren

1. Auswirkung Anzahl der Kommunikationsrunden (bei insgesamt gleich vielen Epochen)

    Behauptung: Mehr Kommunikationsrunden bringen höhere Performance. Zu dem Preis eines größeren Datenverkehrs
    WAHR / FALSCH? [ ]

    60 Epochen Training:
    Keine Komm-Runde: sehr schlecht
    eine Komm-Runde: schon deutlich besser
    danach nur geringfügig besser mit jeder Komm-Runde
    insgesamt alles min 10% schlechter als Central

    300 Epochen Training:
    ?

    Fazit: Abwägung höherer Datenverkehr oder mehr Performance

---------------------------------------------------------------------------------------------------------------

2. Welche Parameter schicken? / Wie viele?
    ⇾ Methoden in Research suchen
    ⇾ Testen und vergleichen: Wie viel schlechter und wie viel weniger Daten? → Lohnt es sich?

    Idee 1: Dropout Gradienten nicht mitschicken [Fazit: sehr schwierig]
    Idee 2: Nur bestimmte schichten schicken → Einträge aus den dictionaries löschen (bestimmte keys)
    Idee 3: Maske → Maske als 0/1 - Matrix und multiplizieren. → Anteil 1sen ist Prozentanteil der geschickten Daten
        TODO: EXTRA Variationen: Teile jedes mal Random Maske oder Teile statische Maske nur am Anfang

---------------------------------------------------------------------------------------------------------------

3. Bei reduziertem Datenverkehr mehr Kommunikationsrunden für eine insgesamt bessere Performance?